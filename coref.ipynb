{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install EbookLib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUUQorl2WaAi",
        "outputId": "327b127e-9b07-41c0-c9cb-13ab0ee966ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting EbookLib\n",
            "  Downloading EbookLib-0.18.tar.gz (115 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/115.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/115.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from EbookLib) (4.9.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from EbookLib) (1.16.0)\n",
            "Building wheels for collected packages: EbookLib\n",
            "  Building wheel for EbookLib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for EbookLib: filename=EbookLib-0.18-py3-none-any.whl size=38778 sha256=7ddd05b1cd793f4cad7a372b44b2b553697d4c2608aa17e683fa78c6018e141a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/38/cc/a3728bb72a315d9d8766fb71d362136372066fc25ad838f8fa\n",
            "Successfully built EbookLib\n",
            "Installing collected packages: EbookLib\n",
            "Successfully installed EbookLib-0.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ebooklib\n",
        "from ebooklib import epub\n",
        "from bs4 import BeautifulSoup\n",
        "import re"
      ],
      "metadata": {
        "id": "vmvAOCRYQ9au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ou7J_F3PYKS",
        "outputId": "a84d8a75-bd08-48f6-db95-0784366112a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to EPUB file in Google Drive\n",
        "#epub_path = '/content/drive/My Drive/TORI_dataset/Gatsby.epub'\n",
        "#epub_path = '/content/drive/My Drive/TORI_dataset/blue_castle.epub'\n",
        "#epub_path = '/content/drive/My Drive/TORI_dataset/little_woman.epub'\n",
        "#epub_path = '/content/drive/My Drive/TORI_dataset/alice_wonderland.epub'\n",
        "epub_path = '/content/drive/My Drive/TORI_dataset/enchanted_april.epub'\n",
        "#epub_path = '/content/drive/My Drive/TORI_dataset/A_roomtour.epub'\n",
        "\n",
        "# Read the EPUB file\n",
        "book = epub.read_epub(epub_path)"
      ],
      "metadata": {
        "id": "050XxmWoVzNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ebooklib\n",
        "from ebooklib import epub\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "\n",
        "# Function to validate the EPUB structure\n",
        "def is_valid(epub_book):\n",
        "    \"\"\"Validates the structure of an EPUB book.\"\"\"\n",
        "    if not epub_book.toc:\n",
        "        print('Table of content is missing')\n",
        "        return False\n",
        "    if not epub_book.spine:\n",
        "        print('Spine is missing')\n",
        "        return False\n",
        "\n",
        "    # Check for stylesheets\n",
        "    stylesheets = [item for item in epub_book.get_items_of_type(ebooklib.ITEM_STYLE)]\n",
        "    if len(stylesheets) == 0:\n",
        "        print('No stylesheets')\n",
        "        return False\n",
        "\n",
        "    # Ensure chapters are not empty\n",
        "    for item in epub_book.spine:\n",
        "        if isinstance(item, epub.EpubHtml):\n",
        "            if not item.content.strip():\n",
        "                print(f\"Invalid book: Chapter '{item.get_id()}' is empty.\")\n",
        "                return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# Validate the book\n",
        "if is_valid(book):\n",
        "    print(\"The EPUB book is valid.\")\n",
        "else:\n",
        "    print(\"The EPUB book is not valid.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHB4CxpLVs1a",
        "outputId": "96c3e187-fae1-4de0-81b9-63e9577919dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The EPUB book is valid.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert a chapter object into a string, filtering out non-textual elements\n",
        "def chapter_to_str(chapter):\n",
        "    \"\"\"Converts a chapter object into a string, filtering out non-textual elements.\"\"\"\n",
        "    soup = BeautifulSoup(chapter.get_body_content(), 'html.parser')\n",
        "    # Filtering out script and style elements\n",
        "    for script_or_style in soup([\"script\", \"style\"]):\n",
        "        script_or_style.decompose()\n",
        "    text = ' '.join(para.get_text(strip=True) for para in soup.find_all('p'))\n",
        "    return text\n",
        "\n",
        "# Function to determine if the item is a Gutenberg cover wrapper\n",
        "def is_wrapper(id):\n",
        "    \"\"\"Determine if the item is the Gutenberg cover wrapper.\"\"\"\n",
        "    patterns = [r'coverpage', r'wrapper']\n",
        "    return any(re.search(pattern, id, re.IGNORECASE) for pattern in patterns)\n"
      ],
      "metadata": {
        "id": "OgkWpZ7-V1Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_gutenberg_intro(text):\n",
        "    \"\"\"Checks if text contains common Gutenberg intro phrases.\"\"\"\n",
        "    patterns = [r'Project Gutenberg', r'\\bEBook\\b', r'\\blicense\\b', r'This eBook is for the use of']\n",
        "    return any(re.search(pattern, text, re.IGNORECASE) for pattern in patterns)"
      ],
      "metadata": {
        "id": "1VOCxdTnFlzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced function to dynamically extract descriptions, including handling pronouns\n",
        "def enhanced_extract_descriptions(text, characters, appearance_words):\n",
        "    sentences = text.split('.')  # Split the text into sentences\n",
        "    character_descriptions = {char: [] for char in characters}\n",
        "    last_mentioned_character = None\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()\n",
        "        for i, word in enumerate(words):\n",
        "            # Check if the word is a character name\n",
        "            for char in characters:\n",
        "                if char in sentence:\n",
        "                    last_mentioned_character = char\n",
        "\n",
        "            # Handle coreference with pronouns\n",
        "            if word.lower() in ['he', 'she', 'his', 'her', 'him', 'herself', 'himself'] and last_mentioned_character:\n",
        "                if i + 1 < len(words) and words[i + 1] in appearance_words:\n",
        "                    phrase = f\"{words[i]} {words[i + 1]}\"\n",
        "                    character_descriptions[last_mentioned_character].append(phrase)\n",
        "\n",
        "            # Handle direct descriptions (like \"long hair\")\n",
        "            if word in appearance_words and i > 0:\n",
        "                previous_word = words[i-1]\n",
        "                if previous_word.isalpha() and previous_word not in appearance_words:\n",
        "                    phrase = f\"{previous_word} {word}\"\n",
        "                    if last_mentioned_character:\n",
        "                        character_descriptions[last_mentioned_character].append(phrase)\n",
        "\n",
        "    return character_descriptions"
      ],
      "metadata": {
        "id": "g9AvS4ORQ17H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved filtering function to remove generic phrases\n",
        "def improved_filter_phrases(character_descriptions):\n",
        "    filtered_descriptions = {}\n",
        "    for character, phrases in character_descriptions.items():\n",
        "        filtered_phrases = []\n",
        "        for phrase in phrases:\n",
        "            words = phrase.split()\n",
        "            # Remove phrases that start with pronouns and are too generic\n",
        "            if len(words) > 1 and not re.match(r'\\b(her|his|their|your|my|our)\\b', words[0]):\n",
        "                filtered_phrases.append(phrase)\n",
        "        if filtered_phrases:\n",
        "            filtered_descriptions[character] = filtered_phrases\n",
        "    return filtered_descriptions"
      ],
      "metadata": {
        "id": "b8HGabsdFwZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to filter out incorrect gender pronouns for each character\n",
        "def correct_gender_pronouns(character_descriptions):\n",
        "    corrected_descriptions = {}\n",
        "    for character, phrases in character_descriptions.items():\n",
        "        if character == [\"Fisher\",\"Wilkins\"]:\n",
        "            filtered_phrases = [phrase for phrase in phrases if not re.match(r'\\bhis\\b', phrase)]\n",
        "        elif character in [\"Wilkins\"]:\n",
        "            filtered_phrases = [phrase for phrase in phrases if not re.match(r'\\bher\\b', phrase)]\n",
        "        else:\n",
        "            filtered_phrases = phrases\n",
        "        if filtered_phrases:\n",
        "            corrected_descriptions[character] = filtered_phrases\n",
        "    return corrected_descriptions"
      ],
      "metadata": {
        "id": "Vm7ON5B1FzD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each chapter in the book\n",
        "def process_chapters(book):\n",
        "    chapters_with_descriptions = {}\n",
        "    for spine_item in book.spine:\n",
        "        item_id = spine_item[0] if isinstance(spine_item, tuple) else spine_item\n",
        "        item = book.get_item_with_id(item_id)\n",
        "\n",
        "        if isinstance(item, epub.EpubHtml) and not is_wrapper(item.get_id()):\n",
        "            chapter_text = chapter_to_str(item)\n",
        "            if not is_gutenberg_intro(chapter_text):  # Skip Gutenberg intro parts\n",
        "                chapter_title = f\"Chapter {item.get_id()}\"\n",
        "                character_descriptions = enhanced_extract_descriptions(chapter_text, characters_of_interest, appearance_words)\n",
        "                filtered_descriptions = improved_filter_phrases(character_descriptions)\n",
        "                gender_corrected_descriptions = correct_gender_pronouns(filtered_descriptions)\n",
        "                chapters_with_descriptions[chapter_title] = gender_corrected_descriptions\n",
        "    return chapters_with_descriptions"
      ],
      "metadata": {
        "id": "zvH2sJb9bTvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and parse the relationships.txt file\n",
        "appearance_words = set()\n",
        "\n",
        "# Path to the relationships.txt file\n",
        "relationships_path = '/content/drive/My Drive/TORI_wordnet/enchanted_april/relationships.txt'\n",
        "\n",
        "# Read and process the file\n",
        "with open(relationships_path, 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Use regular expressions to find all appearance-related terms\n",
        "matches = re.findall(r'synonyms:\\s*\\[(.*?)\\]', content)\n",
        "\n",
        "# Process and clean up the words\n",
        "for match in matches:\n",
        "    words = re.findall(r\"'(.*?)'\", match)\n",
        "    appearance_words.update(words)\n",
        "\n",
        "# Convert the set to a sorted list for consistency\n",
        "appearance_words = sorted(list(appearance_words))\n",
        "\n",
        "print(\"Appearance-related words:\", appearance_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_6ujBUQLwX5",
        "outputId": "3374d78e-f2e7-4375-e88a-47a9a2821b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appearance-related words: ['affection', 'affectionateness', 'anatomy', 'animal foot', 'arm', 'arms', 'aspect', 'auricle', 'back', 'back talk', 'back up', 'backbone', 'backrest', 'backtalk', 'backward', 'backwards', 'base', 'bet on', 'binding', 'blazon', 'blazonry', 'bod', 'boldness', 'book binding', 'bosom', 'brain', 'branch', 'brass', 'bridge player', 'brow', 'build', 'build up', 'buttock', 'calculate', 'capitulum', 'caput', 'case', 'center', 'centre', 'channelise', 'channelize', 'chassis', 'cheek', 'chief', 'cipher', 'coat of arms', 'compute', 'confront', 'core', 'count on', 'cover', 'custody', 'cypher', 'deal', 'design', 'digit', 'direct', 'dorsum', 'drumhead', 'ear', 'endorse', 'enter', 'envision', 'essence', 'estimate', 'expression', 'eye', 'eyeball', 'eyes', 'face', 'face up', 'facial expression', 'fancy', 'fig', 'figure', 'figure of speech', 'flesh', 'fondness', 'font', 'foot', 'forecast', 'forefront', 'forehead', 'foreland', 'form', 'fortify', 'foundation', 'fount', 'fountainhead', 'frame', 'front', 'frontal bone', 'ft', 'fundament', 'fuzz', 'gage', 'game', 'gird', 'gist', 'give', 'grimace', 'groundwork', 'guide', 'hair', 'haircloth', 'hairsbreadth', 'hand', 'hands', 'handwriting', 'head', 'head teacher', 'head up', 'head word', 'header', 'heading', 'headland', 'headspring', 'headway', 'heart', 'heart and soul', 'helping hand', 'hind', 'hinder', 'hired hand', 'hired man', 'human body', 'human face', 'human foot', 'image', 'impertinence', 'implements of war', 'impudence', 'indorse', 'infantry', 'invertebrate foot', 'inwardness', 'kernel', 'lead', 'leg', 'legs', 'limb', 'lip', 'look', 'maneuver', 'manoeuver', 'manoeuvre', 'manpower', 'manus', 'marrow', 'material body', 'meat', 'men', 'metrical foot', 'metrical unit', 'mettle', 'middle', 'mind', 'mitt', 'mouth', 'mouthpiece', 'munition', 'name', 'nerve', 'nitty-gritty', 'nous', 'nub', 'number', 'oculus', 'optic', 'oral cavity', 'oral fissure', 'oral sex', 'os frontale', 'pass', 'pass on', 'pattern', 'paw', 'peg', 'pegleg', 'pes', 'philia', 'physical body', 'physique', 'picture', 'pilus', 'pinna', 'pith', 'plump for', 'plunk for', 'point', 'present', 'principal', 'project', 'promontory', 'psyche', 'public figure', 'pump', 'punt', 'question', 'rachis', 'ramification', 'reach', 'read/write head', 'rear', 'rearward', 'rearwards', 'reckon', 'rima oris', 'sass', 'sassing', 'school principal', 'script', 'second', 'see', 'shape', 'side', 'sleeve', 'soma', 'speak', 'spike', 'spinal column', 'spine', 'spirit', 'spunk', 'stage', 'stake', 'steer', 'straits', 'subdivision', 'substance', 'substructure', 'sum', 'support', 'talk', 'tenderness', 'ticker', 'tomentum', 'top dog', 'trope', 'turn over', 'typeface', 'understructure', 'utter', 'verbalise', 'verbalize', 'vertebral column', 'visualise', 'visualize', 'warmheartedness', 'warmness', 'weapon', 'weapon system', 'weaponry', 'weapons system', 'whisker', 'wooden leg', 'work force', 'work out', 'workforce']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "characters_of_interest = [\"Fisher\",\"Wilkins\",\"Wilkins\"]"
      ],
      "metadata": {
        "id": "V4NEooUVV-Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_chapters_with_enhanced_descriptions = process_chapters(book)"
      ],
      "metadata": {
        "id": "2k7YKDR0CwVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final organized data to a text file\n",
        "#output_path_final_enhanced = '/content/drive/My Drive/Coref/blue_castle_organized_descriptions_by_chapter.txt'\n",
        "#output_path_final_enhanced = '/content/drive/My Drive/Coref/gatsby_organized_descriptions_by_chapter.txt'\n",
        "#output_path_final_enhanced = '/content/drive/My Drive/Coref/little_woamn_organized_descriptions_by_chapter.txt'\n",
        "#output_path_final_enhanced = '/content/drive/My Drive/Coref/alice_wonderland_organized_descriptions_by_chapter.txt'\n",
        "output_path_final_enhanced = '/content/drive/My Drive/Coref/april_organized_descriptions_by_chapter.txt'\n",
        "#output_path_final_enhanced = '/content/drive/My Drive/Coref/roomtour_organized_descriptions_by_chapter.txt'\n",
        "\n",
        "\n",
        "with open(output_path_final_enhanced, 'w') as file:\n",
        "    for chapter, characters in final_chapters_with_enhanced_descriptions.items():\n",
        "        file.write(f\"{chapter}:\\n\")\n",
        "        for character, descriptions in characters.items():\n",
        "            file.write(f\"  {character}:\\n\")\n",
        "            for description in descriptions:\n",
        "                file.write(f\"    - {description}\\n\")\n",
        "        file.write(\"\\n\")"
      ],
      "metadata": {
        "id": "jzbKtEaDC5mQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}